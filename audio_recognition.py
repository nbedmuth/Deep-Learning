# -*- coding: utf-8 -*-
"""hw1_p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19nwLPqHkz8Y-WFgwByF_zhJtwvyViKg7
"""

import sys
import numpy as np
import torch
import torch.optim as optim
from torch.utils import data
from torch.autograd import Variable
import torch.nn as nn
from torch.utils import data
import matplotlib.pyplot as plt
import time

cuda = torch.cuda.is_available()

# +
m = nn.Conv1d(16, 33, 3, stride=2)

input = torch.randn(20, 16, 50)
input[0][0].shape

output = m(input)
#print(output.shape)

A = np.arange(30.).reshape(2,3,5)
print(A)
B = np.arange(24.).reshape(3,4,2)
print(B)
C = np.tensordot(A,B, axes = ([1,0],[0,2]))
print(C)

# +
dev_x = np.load('train.npy', allow_pickle=True)
dev_y = np.load('train_labels.npy', allow_pickle=True)
test_y = np.load('test_labels.npy', allow_pickle=True)
test_x= np.load('test.npy', allow_pickle=True)

halfy=dev_y[0:12500]
half=dev_x[0:12500]
#print("check1")
saved_model_path='./saved_model.pt'
# -

# batch size = 2K+1
K = 13

class MyDataset(data.Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y
        
        no_of_ut = len(self.x)
        
        for i in range(no_of_ut):
            self.x[i] = np.pad(self.x[i], ((K,K),(0,0)), 'constant', constant_values=0)      
        self.y = np.concatenate(self.y, axis=0)
        
        self.d = {}
        index = 0
        for i in range(no_of_ut):
            len_utt = len(self.x[i])
            for j in range(len_utt-2*K):
                self.d[index] = (i, j+K)
                index += 1
        
    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, index):
        i, j = self.d[index]
        x = self.x[i][j-K:j+K+1]
        y = self.y[index]
        t_x = torch.tensor(x).float().view(-1)
        t_y = torch.tensor(y).long()
        return t_x, t_y



# +
num_workers=8
#batch size can be 32

#xy=MyDataset(dev_x,dev_y)
xy=MyDataset(half,halfy)
print("dictionary is ready")
test_dataset=MyDataset(test_x,test_y)
data_loader_args=dict(shuffle=True, batch_size=2048, num_workers=num_workers,pin_memory=True)
train_loader=data.DataLoader(xy, **data_loader_args)

test_loader_args = dict(shuffle=False, batch_size=256, num_workers=num_workers, pin_memory=True) if cuda\
                    else dict(shuffle=False, batch_size=256)
test_loader = data.DataLoader(test_dataset, **test_loader_args) 


# +
 

class Simple_MLP(nn.Module):
    def __init__(self, size_list):
        super(Simple_MLP, self).__init__()
        layers = []
        self.size_list = size_list
        for i in range(len(size_list) - 2):
            layers.append(nn.Linear(size_list[i],size_list[i+1]))
            layers.append(torch.nn.BatchNorm1d(size_list[i+1],eps=1e-05, momentum=0.1,affine=True,track_running_stats=True))
            #layers.append(torch.nn.Dropout(p=0.2))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(size_list[-2], size_list[-1]))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)


# -

model = Simple_MLP([(2*K+1)*40, 2048,1024,1024,1024,512,256, 138])
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5)
print("scheduler was executed")
device = torch.device("cuda" if cuda else "cpu")
model.to(device)

def train_epoch(model, train_loader, criterion, optimizer,scheduler):
    model.train()
    
    running_loss = 0.0
    
    start_time = time.time()
    for batch_idx, (data, target) in enumerate(train_loader):   
        optimizer.zero_grad()   # .backward() accumulates gradients
        data = data.to(device)
        target = target.to(device) # all data & model on same device

        outputs = model(data)
        loss = criterion(outputs, target)
        running_loss += loss.item()

        loss.backward()
        optimizer.step()
    
    scheduler.step(running_loss)
    end_time = time.time()
    
    running_loss /= len(train_loader)
    
    print("scheduler step was executed")
    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')
    return running_loss

def test_model(model, test_loader, criterion,n_epochs):
    with torch.no_grad():
        model.eval()
        
        running_loss = 0.0
        total_predictions = 0.0
        correct_predictions = 0.0
        predict=[]
        for batch_idx, (data, target) in enumerate(test_loader):   
            data = data.to(device)
            target = target.to(device)

            outputs = model(data)
            
            _, predicted = torch.max(outputs.data, 1)
          
            total_predictions += target.size(0)
            correct_predictions += (predicted == target).sum().item()
            predicted=predicted.cpu()
            predicted=predicted.numpy()
            
            predict.extend(predicted)
        
            loss = criterion(outputs, target).detach()
            running_loss += loss.item()
        predict=np.array(predict)
        
        running_loss /= len(test_loader)
        acc = (correct_predictions/total_predictions)*100.0
        print('Testing Loss: ', running_loss)
        print('Testing Accuracy: ', acc, '%')
        with open("test_pred.csv", "w") as f:
            print("id","label",sep=",",file=f)
            for i in range(len(predict)):
                print(i, predict[i], sep=",", file=f)
        return running_loss, acc



# +
n_epochs = 15
Train_loss=[]

try: 
    model.load_state_dict(torch.load(saved_model_path))
    #print("loading")
except:
    #print("saved model not found")
for i in range(1,n_epochs+1):
    print(i)
    train_loss = train_epoch(model, train_loader, criterion, optimizer,scheduler)
    Train_loss.append(train_loss)
    if i in (5,10,15):
        test_loss, test_acc = test_model(model, test_loader, criterion, n_epochs)
    print(str(i)+"done")

torch.save(model.state_dict(), './saved_model.pt')
print("done")

# -


